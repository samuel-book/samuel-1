{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a0802f",
   "metadata": {},
   "source": [
    "# Ensemble model fitting - generate probabilities\n",
    "\n",
    "Aims:\n",
    "\n",
    "* To generate probability outputs for training and test data using logistic regression, random forests, and a neural network.\n",
    "\n",
    "Ensemble modelling combines multiple models in an attempt to increase accuracy. In this test we use the training/test sets with a 10k test cohort. Individual models are built using:\n",
    "\n",
    "* Random Forests (with one-hot encoding of hospital)\n",
    "* Logistic Regression (with one-hot encoding of hospital)\n",
    "* Neural Net (with an embedding layer to encode the hospital)\n",
    "\n",
    "This notebook runs fits for each type of model and returns the predicted probability of receiving thrombolysis for each model.\n",
    "\n",
    "In subsequent notebooks, these individual model outputs will be used as inputs for ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e67f19",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc94b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1d700",
   "metadata": {},
   "source": [
    "## Set up dataframes for probabilitiy outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e801fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_train = pd.DataFrame()\n",
    "probability_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d4a22",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e055822",
   "metadata": {},
   "source": [
    "### Standardise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb91d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Converts all data to a similar scale.\n",
    "    Standardisation subtracts mean and divides by standard deviation\n",
    "    for each feature.\n",
    "    Standardised data will have a mena of 0 and standard deviation of 1.\n",
    "    The training data mean and standard deviation is used to standardise both\n",
    "    training and test set data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler() \n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.transform(X_train)\n",
    "    test_std=sc.transform(X_test)\n",
    "    \n",
    "    return train_std, test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2347a6",
   "metadata": {},
   "source": [
    "### MinMax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71183f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_sc = sc.transform(X_train)\n",
    "    test_sc = sc.transform(X_test)\n",
    "    \n",
    "    return train_sc, test_sc    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f89cec",
   "metadata": {},
   "source": [
    "## Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df67663f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.834\n",
      "Test accuracy: 0.832\n"
     ]
    }
   ],
   "source": [
    "# Read in data\n",
    "train = pd.read_csv('./../data/10k_training_test/cohort_10000_train.csv')\n",
    "test = pd.read_csv('./../data/10k_training_test/cohort_10000_test.csv')\n",
    "\n",
    "# Get X and y\n",
    "X_train = train.drop('S2Thrombolysis', axis=1)\n",
    "X_test = test.drop('S2Thrombolysis', axis=1)\n",
    "y_train = train['S2Thrombolysis']\n",
    "y_test = test['S2Thrombolysis']\n",
    "\n",
    "# One hot encode hospitals\n",
    "X_train_hosp = pd.get_dummies(X_train['StrokeTeam'], prefix = 'team')\n",
    "X_train = pd.concat([X_train, X_train_hosp], axis=1)\n",
    "X_train.drop('StrokeTeam', axis=1, inplace=True)\n",
    "X_test_hosp = pd.get_dummies(X_test['StrokeTeam'], prefix = 'team')\n",
    "X_test = pd.concat([X_test, X_test_hosp], axis=1)\n",
    "X_test.drop('StrokeTeam', axis=1, inplace=True)\n",
    "\n",
    "# Standardise X data\n",
    "X_train_std, X_test_std = standardise_data(X_train, X_test)\n",
    "\n",
    "# Define and Fit model\n",
    "model = LogisticRegression(solver='lbfgs', random_state=42)\n",
    "model.fit(X_train_std, y_train)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_train_probs = model.predict_proba(X_train_std)[:,1]\n",
    "y_test_probs = model.predict_proba(X_test_std)[:,1]\n",
    "\n",
    "# Show accuracy\n",
    "train_class = y_train_probs >= 0.5\n",
    "test_class = y_test_probs >= 0.5\n",
    "accuracy_train = np.mean(y_train == train_class)\n",
    "accuracy_test = np.mean(y_test == test_class)\n",
    "print (f'Training accuracy: {accuracy_train:0.3f}')\n",
    "print (f'Test accuracy: {accuracy_test:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475395e",
   "metadata": {},
   "source": [
    "Add to probability results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10cb993",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_train['logistic_regression'] = y_train_probs\n",
    "probability_test['logistic_regression'] = y_test_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37b4f6",
   "metadata": {},
   "source": [
    "## Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec8bf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.000\n",
      "Test accuracy: 0.842\n"
     ]
    }
   ],
   "source": [
    "# Read in data\n",
    "train = pd.read_csv('./../data/10k_training_test/cohort_10000_train.csv')\n",
    "test = pd.read_csv('./../data/10k_training_test/cohort_10000_test.csv')\n",
    "\n",
    "# Get X and y\n",
    "X_train = train.drop('S2Thrombolysis', axis=1)\n",
    "X_test = test.drop('S2Thrombolysis', axis=1)\n",
    "y_train = train['S2Thrombolysis']\n",
    "y_test = test['S2Thrombolysis']\n",
    "\n",
    "# One hot encode hospitals\n",
    "X_train_hosp = pd.get_dummies(X_train['StrokeTeam'], prefix = 'team')\n",
    "X_train = pd.concat([X_train, X_train_hosp], axis=1)\n",
    "X_train.drop('StrokeTeam', axis=1, inplace=True)\n",
    "X_test_hosp = pd.get_dummies(X_test['StrokeTeam'], prefix = 'team')\n",
    "X_test = pd.concat([X_test, X_test_hosp], axis=1)\n",
    "X_test.drop('StrokeTeam', axis=1, inplace=True)\n",
    "\n",
    "# Define model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100, n_jobs=-1, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_train_probs = model.predict_proba(X_train)[:,1]\n",
    "y_test_probs = model.predict_proba(X_test)[:,1]\n",
    "                                    \n",
    "# Show accuracy\n",
    "train_class = y_train_probs >= 0.5\n",
    "test_class = y_test_probs >= 0.5\n",
    "accuracy_train = np.mean(y_train == train_class)\n",
    "accuracy_test = np.mean(y_test == test_class)\n",
    "print (f'Training accuracy: {accuracy_train:0.3f}')\n",
    "print (f'Test accuracy: {accuracy_test:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "860bf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_train['random_forest'] = y_train_probs\n",
    "probability_test['random_forest'] = y_test_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2478947",
   "metadata": {},
   "source": [
    "## Neural nets (with embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f748d",
   "metadata": {},
   "source": [
    "### Define net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e8b8dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(number_features_patient,\n",
    "             number_features_pathway,\n",
    "             number_features_hospital, \n",
    "             patient_encoding_neurones=1,\n",
    "             pathway_encoding_neurones=1,\n",
    "             hospital_encoding_neurones=1,\n",
    "             expansion=2, \n",
    "             learning_rate=0.003, \n",
    "             dropout=0.5):\n",
    "    \n",
    "    # Clear Tensorflow\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Patient (clinical data) encoding layers\n",
    "    input_patient = layers.Input(shape=number_features_patient)\n",
    "    \n",
    "    # Patient dense layer 1\n",
    "    patient_dense_1 = layers.Dense(\n",
    "        number_features_patient * expansion, activation='relu')(input_patient)\n",
    "    patient_norm_1 = layers.BatchNormalization()(patient_dense_1)\n",
    "    patient_dropout_1 = layers.Dropout(dropout)(patient_norm_1)\n",
    "    \n",
    "    # Patient encoding layer\n",
    "    patient_encoding = layers.Dense(\n",
    "        patient_encoding_neurones, activation='sigmoid', \n",
    "        name='patient_encode')(patient_dropout_1)\n",
    "    \n",
    "    \n",
    "    # Pathway encoding layers\n",
    "    input_pathway = layers.Input(shape=number_features_pathway)\n",
    "    \n",
    "    # pathway dense layer 1\n",
    "    pathway_dense_1 = layers.Dense(\n",
    "        number_features_pathway * expansion, activation='relu')(input_pathway)\n",
    "    pathway_norm_1 = layers.BatchNormalization()(pathway_dense_1)\n",
    "    pathway_dropout_1 = layers.Dropout(dropout)(pathway_norm_1)\n",
    "    \n",
    "    # pathway encoding layer\n",
    "    pathway_encoding = layers.Dense(\n",
    "        pathway_encoding_neurones, activation='sigmoid', \n",
    "        name='pathway_encode')(pathway_dropout_1)\n",
    "    \n",
    "    \n",
    "    # hospital encoding layers\n",
    "    input_hospital = layers.Input(shape=number_features_hospital)\n",
    "    \n",
    "    # hospital encoding layer\n",
    "    hospital_encoding = layers.Dense(\n",
    "        hospital_encoding_neurones, activation='sigmoid', \n",
    "        name='hospital_encode')(input_hospital)    \n",
    "    \n",
    "    # Concatenation layer\n",
    "    concat = layers.Concatenate()(\n",
    "        [patient_encoding, pathway_encoding, hospital_encoding])\n",
    "    \n",
    "    # Outpout (single sigmoid)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(concat)\n",
    "    \n",
    "    # Build net\n",
    "    net = Model(inputs=[\n",
    "        input_patient, input_pathway, input_hospital], outputs=outputs)\n",
    "    \n",
    "    # Compiling model\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    net.compile(loss='binary_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11cfb1f",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11d2d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data subgroups\n",
    "subgroups = pd.read_csv('../data/subnet.csv', index_col='Item')\n",
    "# Get list of clinical items\n",
    "clinical_subgroup = subgroups.loc[subgroups['Subnet']=='clinical']\n",
    "clinical_subgroup = list(clinical_subgroup.index)\n",
    "# Get list of pathway items\n",
    "pathway_subgroup = subgroups.loc[subgroups['Subnet']=='pathway']\n",
    "pathway_subgroup = list(pathway_subgroup.index)\n",
    "# Get list of hospital items\n",
    "hospital_subgroup = subgroups.loc[subgroups['Subnet']=='hospital']\n",
    "hospital_subgroup = list(hospital_subgroup.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a04890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.855\n",
      "Test accuracy: 0.849\n"
     ]
    }
   ],
   "source": [
    "# Read in data\n",
    "train = pd.read_csv('./../data/10k_training_test/cohort_10000_train.csv')\n",
    "test = pd.read_csv('./../data/10k_training_test/cohort_10000_test.csv')\n",
    "\n",
    "# OneHot encode stroke team\n",
    "coded = pd.get_dummies(train['StrokeTeam'])\n",
    "train = pd.concat([train, coded], axis=1)\n",
    "train.drop('StrokeTeam', inplace=True, axis=1)\n",
    "coded = pd.get_dummies(test['StrokeTeam'])\n",
    "test = pd.concat([test, coded], axis=1)\n",
    "test.drop('StrokeTeam', inplace=True, axis=1)\n",
    "\n",
    "# Split into X, y\n",
    "X_train_df = train.drop('S2Thrombolysis',axis=1) \n",
    "y_train_df = train['S2Thrombolysis']\n",
    "X_test_df = test.drop('S2Thrombolysis',axis=1) \n",
    "y_test_df = test['S2Thrombolysis'] \n",
    "\n",
    "# Split train and test data by subgroups\n",
    "X_train_patients = X_train_df[clinical_subgroup]\n",
    "X_test_patients = X_test_df[clinical_subgroup]\n",
    "X_train_pathway = X_train_df[pathway_subgroup]\n",
    "X_test_pathway = X_test_df[pathway_subgroup]\n",
    "X_train_hospitals = X_train_df[hospital_subgroup]\n",
    "X_test_hospitals = X_test_df[hospital_subgroup]\n",
    "\n",
    "# Convert to NumPy\n",
    "X_train = X_train_df.values\n",
    "X_test = X_test_df.values\n",
    "y_train = y_train_df.values\n",
    "y_test = y_test_df.values\n",
    "\n",
    "# Scale data\n",
    "X_train_patients_sc, X_test_patients_sc = \\\n",
    "    scale_data(X_train_patients, X_test_patients)\n",
    "\n",
    "X_train_pathway_sc, X_test_pathway_sc = \\\n",
    "    scale_data(X_train_pathway, X_test_pathway)\n",
    "\n",
    "X_train_hospitals_sc, X_test_hospitals_sc = \\\n",
    "    scale_data(X_train_hospitals, X_test_hospitals)\n",
    "\n",
    "# Define network\n",
    "number_features_patient = X_train_patients_sc.shape[1]\n",
    "number_features_pathway = X_train_pathway_sc.shape[1]\n",
    "number_features_hospital = X_train_hospitals_sc.shape[1]\n",
    "\n",
    "model = make_net(\n",
    "    number_features_patient, \n",
    "    number_features_pathway, \n",
    "    number_features_hospital)\n",
    "\n",
    "# Define save checkpoint callback (only save if new best validation results)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    'model_checkpoint_1d.h5', save_best_only=True)\n",
    "\n",
    "# Define early stopping callback: Stop when no validation improvement\n",
    "# Restore weights to best validation accuracy\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    patience=50, restore_best_weights=True)\n",
    "\n",
    "# Train model (including class weights)\n",
    "history = model.fit(\n",
    "    [X_train_patients_sc, X_train_pathway_sc, X_train_hospitals_sc],\n",
    "    y_train,\n",
    "    epochs=5000,\n",
    "    batch_size=32,\n",
    "    validation_data=(\n",
    "        [X_test_patients_sc, X_test_pathway_sc, X_test_hospitals_sc], \n",
    "        y_test),\n",
    "    verbose=0,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_train_probs = model.predict(\n",
    "        [X_train_patients_sc, X_train_pathway_sc, X_train_hospitals_sc])\n",
    "y_test_probs = model.predict(\n",
    "        [X_test_patients_sc, X_test_pathway_sc, X_test_hospitals_sc])\n",
    "\n",
    "# Flatten arrays\n",
    "y_train_probs = y_train_probs.flatten()\n",
    "y_test_probs = y_test_probs.flatten()\n",
    "\n",
    "# Show accuracy\n",
    "train_class = y_train_probs >= 0.5\n",
    "test_class = y_test_probs >= 0.5\n",
    "accuracy_train = np.mean(y_train == train_class)\n",
    "accuracy_test = np.mean(y_test == test_class)\n",
    "print (f'Training accuracy: {accuracy_train:0.3f}')\n",
    "print (f'Test accuracy: {accuracy_test:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56e298d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_train['neural_net'] = y_train_probs\n",
    "probability_test['neural_net'] = y_test_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf69d2ff",
   "metadata": {},
   "source": [
    "# Add actual use of thrombolysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "909d0f89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'probability_ttest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3e1c94c9b6b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprobability_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'S2Thrombolysis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprobability_ttest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'S2Thrombolysis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'probability_ttest' is not defined"
     ]
    }
   ],
   "source": [
    "probability_train['actual'] = train['S2Thrombolysis']\n",
    "probability_ttest['actual'] = test['S2Thrombolysis']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde3922",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498fb942",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_train.to_csv(\n",
    "    './individual_model_output/probabilities_train.csv', index = False)\n",
    "probability_test.to_csv(\n",
    "    './individual_model_output/probabilities_test.csv', index=False)                      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
