{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble combine with a neural network\n",
    "\n",
    "Aim: To combine output from logistic regression, random forests, and neural network models in a neural network model, with or without original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# TensorFlow api model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_sc = sc.transform(X_train)\n",
    "    test_sc = sc.transform(X_test)\n",
    "    \n",
    "    return train_sc, test_sc    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(number_features, expansion=2, learning_rate=0.003, dropout=0.5):\n",
    "    \n",
    "    # Clear Tensorflow\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=number_features)\n",
    "    \n",
    "    # Dense layer 1\n",
    "    dense_1 = layers.Dense(\n",
    "        number_features * expansion, activation='relu')(inputs)\n",
    "    norm_1 = layers.BatchNormalization()(dense_1)\n",
    "    dropout_1 = layers.Dropout(dropout)(norm_1)\n",
    "    \n",
    "    # Dense layer 2\n",
    "    dense_2 = layers.Dense(\n",
    "        number_features * expansion, activation='relu')(dropout_1)\n",
    "    norm_2 = layers.BatchNormalization()(dense_2)\n",
    "    dropout_2 = layers.Dropout(dropout)(norm_2)    \n",
    " \n",
    "    # Outpout (single sigmoid)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(dropout_2)\n",
    "    \n",
    "    # Build net\n",
    "    net = Model(inputs, outputs)\n",
    "    \n",
    "    # Compiling model\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    net.compile(loss='binary_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model using original training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.862\n",
      "Test accuracy: 0.834\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('./../data/10k_training_test/cohort_10000_train.csv')\n",
    "test = pd.read_csv('./../data/10k_training_test/cohort_10000_test.csv')\n",
    "\n",
    "# OneHot encode stroke team\n",
    "coded = pd.get_dummies(train['StrokeTeam'])\n",
    "train = pd.concat([train, coded], axis=1)\n",
    "train.drop('StrokeTeam', inplace=True, axis=1)\n",
    "coded = pd.get_dummies(test['StrokeTeam'])\n",
    "test = pd.concat([test, coded], axis=1)\n",
    "test.drop('StrokeTeam', inplace=True, axis=1)\n",
    "\n",
    "# Split into X, y\n",
    "X_train_df = train.drop('S2Thrombolysis',axis=1) \n",
    "y_train_df = train['S2Thrombolysis']\n",
    "X_test_df = test.drop('S2Thrombolysis',axis=1) \n",
    "y_test_df = test['S2Thrombolysis'] \n",
    "\n",
    "# Convert to NumPy\n",
    "X_train = X_train_df.values\n",
    "X_test = X_test_df.values\n",
    "y_train = y_train_df.values\n",
    "y_test = y_test_df.values\n",
    "\n",
    "# Scale data\n",
    "X_train_sc, X_test_sc = scale_data(X_train, X_test)\n",
    "\n",
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(number_features)\n",
    "\n",
    "# Define save checkpoint callback (only save if new best validation results)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    'model_checkpoint.h5', save_best_only=True)\n",
    "\n",
    "# Define early stopping callback: Stop when no validation improvement\n",
    "# Restore weights to best validation accuracy\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    patience=50, restore_best_weights=True)\n",
    "\n",
    "# Train model (including class weights)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    epochs=5000,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    \n",
    "# Get predicted probabilities\n",
    "y_train_probs = model.predict(X_train_sc)\n",
    "y_train_probs = y_train_probs.flatten()\n",
    "y_test_probs = model.predict(X_test_sc)\n",
    "y_test_probs = y_test_probs.flatten()\n",
    "                                    \n",
    "# Show accuracy\n",
    "train_class = y_train_probs >= 0.5\n",
    "test_class = y_test_probs >= 0.5\n",
    "accuracy_train = np.mean(y_train == train_class)\n",
    "accuracy_test = np.mean(y_test == test_class)\n",
    "print (f'Training accuracy: {accuracy_train:0.3f}')\n",
    "print (f'Test accuracy: {accuracy_test:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model using model probabilities only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.869\n",
      "Test accuracy: 0.835\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "model_probs_train = pd.read_csv(\n",
    "    './individual_model_output/probabilities_train.csv')\n",
    "model_probs_test = pd.read_csv(\n",
    "    './individual_model_output/probabilities_test.csv')\n",
    "\n",
    "# Set up train and test data\n",
    "X_train = model_probs_train\n",
    "X_test = model_probs_test\n",
    "y_train = train['S2Thrombolysis']\n",
    "y_test = test['S2Thrombolysis']\n",
    "\n",
    "# Convert to NumPy\n",
    "X_train = X_train_df.values\n",
    "X_test = X_test_df.values\n",
    "y_train = y_train_df.values\n",
    "y_test = y_test_df.values\n",
    "\n",
    "# Scale data\n",
    "X_train_sc, X_test_sc = scale_data(X_train, X_test)\n",
    "\n",
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(number_features)\n",
    "\n",
    "# Define save checkpoint callback (only save if new best validation results)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    'model_checkpoint.h5', save_best_only=True)\n",
    "\n",
    "# Define early stopping callback: Stop when no validation improvement\n",
    "# Restore weights to best validation accuracy\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    patience=50, restore_best_weights=True)\n",
    "\n",
    "# Train model (including class weights)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    epochs=5000,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    \n",
    "# Get predicted probabilities\n",
    "y_train_probs = model.predict(X_train_sc)\n",
    "y_train_probs = y_train_probs.flatten()\n",
    "y_test_probs = model.predict(X_test_sc)\n",
    "y_test_probs = y_test_probs.flatten()\n",
    "                                    \n",
    "# Show accuracy\n",
    "train_class = y_train_probs >= 0.5\n",
    "test_class = y_test_probs >= 0.5\n",
    "accuracy_train = np.mean(y_train == train_class)\n",
    "accuracy_test = np.mean(y_test == test_class)\n",
    "print (f'Training accuracy: {accuracy_train:0.3f}')\n",
    "print (f'Test accuracy: {accuracy_test:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model using original features and model probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.000\n",
      "Test accuracy: 0.830\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('./../data/10k_training_test/cohort_10000_train.csv')\n",
    "test = pd.read_csv('./../data/10k_training_test/cohort_10000_test.csv')\n",
    "model_probs_train = pd.read_csv(\n",
    "    './individual_model_output/probabilities_train.csv')\n",
    "model_probs_test = pd.read_csv(\n",
    "    './individual_model_output/probabilities_test.csv')\n",
    "\n",
    "# OneHot encode stroke team\n",
    "coded = pd.get_dummies(train['StrokeTeam'])\n",
    "train = pd.concat([train, coded], axis=1)\n",
    "train.drop('StrokeTeam', inplace=True, axis=1)\n",
    "coded = pd.get_dummies(test['StrokeTeam'])\n",
    "test = pd.concat([test, coded], axis=1)\n",
    "test.drop('StrokeTeam', inplace=True, axis=1)\n",
    "\n",
    "# Split into X, y\n",
    "X_train_df = train.drop('S2Thrombolysis',axis=1) \n",
    "y_train_df = train['S2Thrombolysis']\n",
    "X_test_df = test.drop('S2Thrombolysis',axis=1) \n",
    "y_test_df = test['S2Thrombolysis']\n",
    "\n",
    "# Add in model probabilties\n",
    "X_train_df = pd.concat([X_train_df, model_probs_train], axis=1)\n",
    "X_test_df = pd.concat([X_test_df, model_probs_test], axis=1)\n",
    "\n",
    "# Convert to NumPy\n",
    "X_train = X_train_df.values\n",
    "X_test = X_test_df.values\n",
    "y_train = y_train_df.values\n",
    "y_test = y_test_df.values\n",
    "\n",
    "# Scale data\n",
    "X_train_sc, X_test_sc = scale_data(X_train, X_test)\n",
    "\n",
    "# Define network\n",
    "number_features = X_train_sc.shape[1]\n",
    "model = make_net(number_features)\n",
    "\n",
    "# Define save checkpoint callback (only save if new best validation results)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    'model_checkpoint.h5', save_best_only=True)\n",
    "\n",
    "# Define early stopping callback: Stop when no validation improvement\n",
    "# Restore weights to best validation accuracy\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    patience=50, restore_best_weights=True)\n",
    "\n",
    "# Train model (including class weights)\n",
    "history = model.fit(X_train_sc,\n",
    "                    y_train,\n",
    "                    epochs=5000,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test_sc, y_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    \n",
    "# Get predicted probabilities\n",
    "y_train_probs = model.predict(X_train_sc)\n",
    "y_train_probs = y_train_probs.flatten()\n",
    "y_test_probs = model.predict(X_test_sc)\n",
    "y_test_probs = y_test_probs.flatten()\n",
    "                                    \n",
    "# Show accuracy\n",
    "train_class = y_train_probs >= 0.5\n",
    "test_class = y_test_probs >= 0.5\n",
    "accuracy_train = np.mean(y_train == train_class)\n",
    "accuracy_test = np.mean(y_test == test_class)\n",
    "print (f'Training accuracy: {accuracy_train:0.3f}')\n",
    "print (f'Test accuracy: {accuracy_test:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "* Including in model probability outputs from previous logistic regression, random forests, and neural networks, did not improve accuracy of the model compared to fitting a random forests model just on the original data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
