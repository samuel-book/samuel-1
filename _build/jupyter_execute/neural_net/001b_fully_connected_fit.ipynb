{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected TensorFlow model - Train and save models\n",
    "\n",
    "## Aims\n",
    "\n",
    "* To train and save fully connected models for the 5-fold training/test data sets\n",
    "\n",
    "## Basic methodology\n",
    "\n",
    "* Models are fitted to previously split training and test data sets. \n",
    "\n",
    "* MinMax scaling is used (all features are scaled 0-1 based on the fature min/max).\n",
    "\n",
    "* Model has two hidden layers, each with the number of neurones being 2x the number of features. Prior studies show performance of the network is similar across all models with this complexity or more. A dropout value of 0.5 is used based on previous exploration.\n",
    "\n",
    "* A batch size of 32 is used (\"Friends don't let friends use minibatches larger than 32\". Yan LeCun on paper: arxiv.org/abs/1804.07612)\n",
    "\n",
    "* 30 Training epochs are used as previously established.\n",
    "\n",
    "Model structure:\n",
    "\n",
    "* Input layer\n",
    "* Dense layer (# neurones = 2x features, ReLu activation)\n",
    "* Batch normalisation \n",
    "* Dropout layer\n",
    "* Dense layer (# neurones = 2x features, ReLu activation)\n",
    "* Batch normalisation \n",
    "* Dropout layer\n",
    "* Output layer (single sigmoid activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './saved_models/fully_connected/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn for pre-processing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# TensorFlow api model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to scale data (minmax scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_sc = sc.transform(X_train)\n",
    "    test_sc = sc.transform(X_test)\n",
    "    \n",
    "    return train_sc, test_sc    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(number_features, expansion=2, learning_rate=0.003, dropout=0.5):\n",
    "    \n",
    "    # Clear Tensorflow\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=number_features)\n",
    "    \n",
    "    # Dense layer 1\n",
    "    dense_1 = layers.Dense(\n",
    "        number_features * expansion, activation='relu')(inputs)\n",
    "    norm_1 = layers.BatchNormalization()(dense_1)\n",
    "    dropout_1 = layers.Dropout(dropout)(norm_1)\n",
    "    \n",
    "    # Dense layer 2\n",
    "    dense_2 = layers.Dense(\n",
    "        number_features * expansion, activation='relu')(dropout_1)\n",
    "    norm_2 = layers.BatchNormalization()(dense_2)\n",
    "    dropout_2 = layers.Dropout(dropout)(norm_2)    \n",
    " \n",
    "    # Outpout (single sigmoid)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(dropout_2)\n",
    "    \n",
    "    # Build net\n",
    "    net = Model(inputs, outputs)\n",
    "    \n",
    "    # Compiling model\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    net.compile(loss='binary_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0: Accuracy train 0.858 Accuracy test 0.844\n",
      "Model 1: Accuracy train 0.857 Accuracy test 0.846\n",
      "Model 2: Accuracy train 0.858 Accuracy test 0.843\n",
      "Model 3: Accuracy train 0.859 Accuracy test 0.848\n",
      "Model 4: Accuracy train 0.857 Accuracy test 0.842\n"
     ]
    }
   ],
   "source": [
    "# Loop through 5 k-folds\n",
    "for k in range(5):\n",
    "    \n",
    "    # Load data\n",
    "    train = pd.read_csv(f'../data/kfold_5fold/train_{k}.csv')\n",
    "    test = pd.read_csv(f'../data/kfold_5fold/test_{k}.csv')\n",
    "    \n",
    "    # OneHot encode stroke team\n",
    "    coded = pd.get_dummies(train['StrokeTeam'])\n",
    "    train = pd.concat([train, coded], axis=1)\n",
    "    train.drop('StrokeTeam', inplace=True, axis=1)\n",
    "    coded = pd.get_dummies(test['StrokeTeam'])\n",
    "    test = pd.concat([test, coded], axis=1)\n",
    "    test.drop('StrokeTeam', inplace=True, axis=1)\n",
    "    \n",
    "    # Split into X, y\n",
    "    X_train_df = train.drop('S2Thrombolysis',axis=1) \n",
    "    y_train_df = train['S2Thrombolysis']\n",
    "    X_test_df = test.drop('S2Thrombolysis',axis=1) \n",
    "    y_test_df = test['S2Thrombolysis'] \n",
    "        \n",
    "    # Convert to NumPy\n",
    "    X_train = X_train_df.values\n",
    "    X_test = X_test_df.values\n",
    "    y_train = y_train_df.values\n",
    "    y_test = y_test_df.values\n",
    "    \n",
    "    # Scale data\n",
    "    X_train_sc, X_test_sc = scale_data(X_train, X_test)\n",
    "    \n",
    "    # Define network\n",
    "    number_features = X_train_sc.shape[1]\n",
    "    model = make_net(number_features)\n",
    "    \n",
    "    # Define save checkpoint callback (only save if new best validation results)\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "        'model_checkpoint.h5', save_best_only=True)\n",
    "    \n",
    "    # Define early stopping callback: Stop when no validation improvement\n",
    "    # Restore weights to best validation accuracy\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "        patience=50, restore_best_weights=True)\n",
    "    \n",
    "    # Train model (including class weights)\n",
    "    history = model.fit(X_train_sc,\n",
    "                        y_train,\n",
    "                        epochs=30,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_test_sc, y_test),\n",
    "                        verbose=0)\n",
    "    #                   callbacks=[checkpoint_cb, early_stopping_cb]\n",
    "    \n",
    "    ### Test model\n",
    "    probability = model.predict(X_train_sc)\n",
    "    y_pred_train = probability >= 0.5\n",
    "    y_pred_train = y_pred_train.flatten()\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    print(f'Model {k}: Accuracy train {accuracy_train:0.3f}', end=' ')\n",
    "    \n",
    "    probability = model.predict(X_test_sc)\n",
    "    y_pred_test = probability >= 0.5\n",
    "    y_pred_test = y_pred_test.flatten()\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    print(f'Accuracy test {accuracy_test:0.3f}')\n",
    "    \n",
    "    # save model\n",
    "    filename = f'{path}model_{str(k)}.h5'\n",
    "    model.save(filename);\n",
    "    \n",
    "    # Remove model\n",
    "    del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}